import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import fitz  # PyMuPDF
import io


# === 1Ô∏è‚É£ LER O CSV CORRETAMENTE ===
csv_path = "../data/pages2.csv"

# Detectar automaticamente a linha onde come√ßa o cabe√ßalho
with open(csv_path, "r", encoding="utf-8") as f:
    lines = f.readlines()

# Encontrar a primeira linha que contenha "Sec√ß√£o"
header_line = next(i for i, line in enumerate(lines) if "Sec√ß√£o" in line)

# Ler o CSV a partir dessa linha
df = pd.read_csv(csv_path, sep=";", header=header_line)
df = df.fillna(method="ffill")

# Lista para guardar as p√°ginas extra√≠das
pages = []

headers = {"User-Agent": "Mozilla/5.0 (AISEL academic bot)"}


def limpar_texto(texto):
    """Remove espa√ßos e quebras desnecess√°rias"""
    return " ".join(texto.split())


# === 2Ô∏è‚É£ LER AS P√ÅGINAS HTML ===
for i, row in df.iterrows():
    secao = str(row.get("Sec√ß√£o", ""))
    subsecao = str(row.get("Subsec√ß√£o", ""))
    url = str(row.get("URL completo", ""))
    titulo = str(row.get("T√≠tulo", ""))

    # Ignora URLs vazios ou inv√°lidos
    if not url or not url.startswith("http"):
        print(f"‚ö†Ô∏è URL inv√°lido (linha {i}): '{url}'")
        continue

    # Ignora PDFs (tratamos depois)
    if "pdf" in url.lower():
        continue

    print(f"üìñ [{secao or '?'} > {subsecao or '?'}] A ler: {url}")

    try:
        resposta = requests.get(url, headers=headers, timeout=20)
        if resposta.status_code != 200:
            print(f"‚ö†Ô∏è Erro {resposta.status_code} ao abrir {url}")
            continue

        soup = BeautifulSoup(resposta.text, "html.parser")

        # Extrai o conte√∫do principal (<main>) ou todo o HTML
        main = soup.select_one("main") or soup

        # Extrai par√°grafos
        paragrafos = [p.get_text(" ", strip=True) for p in main.find_all("p")]
        texto = limpar_texto(" ".join(paragrafos))

        # Caso o texto seja curto, adiciona headings
        if len(texto) < 150:
            heads = [h.get_text(" ", strip=True) for h in soup.select("h1,h2,h3")]
            texto = limpar_texto(" ".join(heads + paragrafos))

        pages.append({
            "sec√ß√£o": secao,
            "subsec√ß√£o": subsecao,
            "url": url,
            "titulo": titulo,
            "conteudo": texto
        })

        print(f"‚úÖ Extra√≠do: {len(texto)} caracteres")
        time.sleep(1)

    except Exception as e:
        print(f"‚ùå Erro a processar {url}: {e}")

# === 4Ô∏è‚É£ GUARDAR O RESULTADO EM JSON ===
output_path = "../data/pages.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(pages, f, ensure_ascii=False, indent=2)

print("\nüéâ Ficheiro pages.json criado com sucesso!")
print(f"Total de p√°ginas extra√≠das: {len(pages)}")

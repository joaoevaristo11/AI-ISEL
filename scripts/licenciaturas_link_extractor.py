import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import pandas as pd
import time
from collections import defaultdict

# ==============================
# 1Ô∏è‚É£ Configura√ß√µes
# ==============================
BASE_URL = "https://www.isel.pt/cursos/licenciaturas"
DOMAIN = urlparse(BASE_URL).netloc
HEADERS = {"User-Agent": "Mozilla/5.0 (AI-ISEL academic crawler)"}

print(f"üöÄ A aceder √† p√°gina principal: {BASE_URL}\n")

# ==============================
# 2Ô∏è‚É£ Obter lista de licenciaturas
# ==============================
try:
    resp = requests.get(BASE_URL, headers=HEADERS, timeout=15)
    resp.raise_for_status()
except Exception as e:
    print(f"‚ùå Erro ao aceder √† p√°gina principal: {e}")
    exit(1)

soup = BeautifulSoup(resp.text, "html.parser")

degree_links = []
for a in soup.find_all("a", href=True):
    href = a["href"].strip()
    text = a.get_text(strip=True)
    if not href or not text:
        continue

    full_url = urljoin(BASE_URL, href)
    if "/curso/licenciatura/" in full_url and DOMAIN in full_url:
        degree_links.append((text, full_url))

# Remover duplicados mantendo a ordem
degree_links = list(dict.fromkeys(degree_links))

print(f"üéì Total de licenciaturas encontradas: {len(degree_links)}\n")

# ==============================
# 3Ô∏è‚É£ Fun√ß√£o auxiliar ‚Äì Ignorar menus globais
# ==============================
def is_in_navigation(a_tag, base_path):
    # Ignorar menus globais (header/nav/footer/aside ou role="navigation")
    if a_tag.find_parent(['header', 'nav', 'footer', 'aside']):
        return True

    role = a_tag.get('role', '')
    if role and 'navigation' in role.lower():
        return True

    parent = a_tag.parent
    while parent:
        cls = " ".join(parent.get('class', []))
        # ‚ö†Ô∏è S√≥ ignorar se for um menu global ‚Äî evitar remover menus internos do curso
        if any(k in cls.lower() for k in ('menu', 'nav', 'navbar', 'header', 'footer', 'cookie', 'skip')):
            # ‚úÖ EXCE√á√ÉO: se estiver dentro de uma √°rea de curso, n√£o ignorar
            if parent.find_parent(class_="banner-curso") or parent.find_parent(class_="views-field-field-menu"):
                return False
            return True
        parent = parent.parent

    # ‚ö†Ô∏è exce√ß√£o: se for link interno de curso (/curso/...), n√£o ignorar
    href = a_tag.get('href', '')
    if href and href.startswith("/curso/"):
        if base_path in href:
            return False

    return False


# ==============================
# 4Ô∏è‚É£ Varredura de cada licenciatura
# ==============================
all_rows = []
pdf_links = set()
links_por_lic = defaultdict(set)

for degree_name, degree_url in degree_links:
    print(f"üîç A processar: {degree_name}")

    try:
        resp = requests.get(degree_url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        print(f"‚ùå Erro ao aceder a {degree_url}: {e}")
        continue

    soup = BeautifulSoup(resp.text, "html.parser")

    # üî∏ Apenas o conte√∫do principal (ignora menus globais automaticamente)
    main_content = (
        soup.select_one("main")
        or soup.select_one(".layout-content")
        or soup.select_one(".region-content")
        or soup
    )

    for a in main_content.find_all("a", href=True):
        href = a["href"].strip()
        if not href or href.startswith("#"):
            continue

        full_url = urljoin(degree_url, href)
        parsed = urlparse(full_url)

        # Ignorar links externos (exceto PDFs)
        if parsed.netloc and parsed.netloc != DOMAIN and not full_url.lower().endswith(".pdf"):
            continue

        # Ignorar links de navega√ß√£o global
        if (not text.strip() and a.find('img')) or is_in_navigation(a, degree_url):
            continue

        # Texto vis√≠vel ou de fallback
        text = a.get_text(strip=True) or a.get('aria-label') or a.get('title') or ""

        # Ignorar links vazios
        if not text.strip() and a.find('img'):
            continue

        # Identificar PDFs
        if full_url.lower().endswith(".pdf"):
            pdf_links.add(full_url)

        links_por_lic[degree_name].add((text or "(sem texto vis√≠vel)", full_url))

    print(f"   ‚úÖ {len(links_por_lic[degree_name])} links √∫nicos recolhidos.\n")
    time.sleep(0.5)

# ==============================
# 5Ô∏è‚É£ Identificar links comuns
# ==============================
for lic, links in links_por_lic.items():
    for text, url in links:
        lic_name = lic
        all_rows.append({
            "Licenciatura": lic_name,
            "Texto": text,
            "URL": url
        })
        
# ==============================
# 6Ô∏è‚É£ Limpeza e grava√ß√£o
# ==============================
df = pd.DataFrame(all_rows).drop_duplicates(subset=["Licenciatura", "URL"])
df = df[df["URL"].str.startswith("http")]
df.sort_values(by=["Licenciatura", "Texto"], inplace=True)

output_excel = "../data/licenciaturas_links_full.xlsx"
df.to_excel(output_excel, index=False)

# ==============================
# 7Ô∏è‚É£ Output final
# ==============================
print("üèÅ Varredura conclu√≠da!\n")
print(f"üéì Total de licenciaturas processadas: {len(degree_links)}")
print(f"üîó Total de links √∫nicos: {len(df)}")
print(f"üìÑ Total de PDFs encontrados: {len(pdf_links)}")
print(f"üìÅ Guardado em: {output_excel}")

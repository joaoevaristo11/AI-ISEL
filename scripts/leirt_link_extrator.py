import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import pandas as pd

# ==============================
# 1Ô∏è‚É£ Configura√ß√µes
# ==============================
BASE_URL = "https://www.isel.pt/curso/licenciatura/licenciatura-em-engenharia-informatica-redes-e-telecomunicacoes"
DOMAIN = urlparse(BASE_URL).netloc
HEADERS = {"User-Agent": "Mozilla/5.0 (AI-ISEL academic crawler)"}

print(f"üöÄ A aceder √† p√°gina: {BASE_URL}\n")

# ==============================
# 2Ô∏è‚É£ Requisi√ß√£o e parsing
# ==============================
try:
    resp = requests.get(BASE_URL, headers=HEADERS, timeout=10)
    resp.raise_for_status()
except Exception as e:
    print(f"‚ùå Erro ao aceder √† p√°gina principal: {e}")
    exit(1)

soup = BeautifulSoup(resp.text, "html.parser")

# ==============================
# 3Ô∏è‚É£ Extra√ß√£o de links
# ==============================
links = []
pdf_links = []

for a in soup.find_all("a", href=True):
    href = a["href"].strip()
    if not href or href.startswith("#"):
        continue

    full_url = urljoin(BASE_URL, href)
    parsed = urlparse(full_url)

    # Ignora links externos (exceto PDFs)
    if parsed.netloc and parsed.netloc != DOMAIN and not full_url.lower().endswith(".pdf"):
        continue

    # Identifica se √© PDF
    if full_url.lower().endswith(".pdf"):
        pdf_links.append(full_url)

    text = a.get_text(strip=True)
    links.append({
        "Texto": text if text else "(sem texto vis√≠vel)",
        "URL": full_url
    })

# ==============================
# 4Ô∏è‚É£ Limpeza e grava√ß√£o
# ==============================
df = pd.DataFrame(links).drop_duplicates(subset=["URL"])
df = df[df["URL"].str.startswith("http")]  # mant√©m apenas URLs v√°lidos

output_csv = "../data/leirt_links_full.csv"
df.to_csv(output_csv, index=False, encoding="utf-8-sig")

# ==============================
# 5Ô∏è‚É£ Output limpo no terminal
# ==============================
total_links = len(df)
total_pdfs = len(set(pdf_links))

print(f"‚úÖ {total_links} links internos encontrados.\n")
print(f"üìÑ Total de PDFs encontrados: {total_pdfs}")
print(f"üìÅ Guardado em: {output_csv}")
